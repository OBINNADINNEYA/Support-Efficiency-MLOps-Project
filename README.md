ğŸš€ Support Efficiency MLOps Using github issues data from Hugging Face

An end-to-end machine learning + MLOps pipeline built on top of 31M GitHub issues.
The goal: predict issue resolution class to improve developer support efficiency and provide deeper insights into what drives resolution times.

Classes:

ğŸŸ¢ Short: â‰¤ 1 day

ğŸŸ¡ Medium: 1â€“7 days

ğŸ”´ Long: > 7 days

All of this is wrapped in a production-grade CI/CD pipeline and deployed to the cloud with Azure Web App.

ğŸ“Š Data Source

We use the BigCode GitHub Issues Dataset
 on Hugging Face.
From 31M+ issues, we extract, clean, and engineer structured features such as:

text_size (issue description length)

comment_count

participants_count

first_response_minutes (time until first maintainer response)

ğŸ” Findings (EDA)

ğŸ““ See the notebook: notebooks/EDA.ipynb

Highlights:

Resolution times are heavily skewed toward â€œLongâ€.

Repository activity and labels correlate strongly with long times.

First response speed emerges as the most critical factor.

Text features (title/description length) have weaker but non-trivial predictive power.

ğŸ¤– Modeling

We evaluated a suite of ML classifiers:

Logistic Regression, Linear SVC, kNN

Tree ensembles: Random Forest, XGBoost, CatBoost, LightGBM

Neural MLP

âœ… Best Model

CatBoost and XGBoost consistently perform best on F1-macro.

Selected via grid search hyperparameter tuning.

Model artifacts are saved in artifacts/model.pkl.

ğŸ“ˆ Explainability with SHAP

We applied SHAP (SHapley Additive exPlanations) to interpret predictions.

Global importance:

first_response_minutes dominates model decisions.

comment_count and participants_count have moderate influence.

text_size contributes the least.

Local explanations: SHAP force plots show why a single issue was predicted â€œShortâ€, â€œMediumâ€, or â€œLongâ€.

This ensures the model is interpretable and provides actionable insights.

ğŸŒ³ Causal Inference

We also ran Causal Forests (econml) to measure the causal effect of fast responses (< 60 minutes) on resolution time.

Results:

Average Treatment Effect (ATE) â‰ˆ -0.05% â†’ Very small causal effect once confounders are controlled.

Heterogeneity:

Effect varies mostly with comment_count.

Tickets with many comments show more variation in benefit.

text_size and participants_count play minimal roles.

Why trees?

Capture non-linearities and interactions.

Handle skewed tabular data.

Provide heterogeneous treatment effects instead of just one number.

This shifts us from correlation (â€œfast responses often close tickets quickerâ€) to causal insight (â€œfast responses alone only marginally shorten resolution timeâ€).

âœ¨ Features

Prediction: Resolution class (Short / Medium / Long)

Explainability: SHAP global + local interpretability

Causal Inference: Tree-based causal methods for intervention analysis

MLOps: MLflow for experiment tracking

Deployment:

FastAPI app for production APIs

Flask app for prototyping / dashboards

Automation: CI/CD with GitHub Actions â†’ build, test, deploy to Azure Web App

ğŸ› ï¸ Quickstart (Local Dev)
1. Clone
git clone https://github.com/OBINNADINNEYA/Support-Efficiency-MLOps-Project.git
cd Support-Efficiency-MLOps-Project

2. Environment
conda create -n support_efficiency python=3.12 -y
conda activate support_efficiency
pip install -r requirements.txt

3. Train
python -m src.pipelines.train_pipeline

4. Predict (batch)
python -m src.pipelines.predict_pipeline --in data/sample.jsonl --out predictions.csv

5. Serve API

FastAPI (production-ready):

uvicorn app.main:app --reload


Flask (prototype UI / dashboard):

python app_flask.py

ğŸ“¦ Artifacts

artifacts/model.pkl â†’ Best trained model

artifacts/preprocessor.pkl â†’ Feature scaling pipeline

artifacts/encoder.pkl â†’ Label encoder for resolution class

artifacts/shap_global.png â†’ SHAP feature importance plot

ğŸ“ˆ Example Output
Best model: CatBoost
Test | Acc: 0.71  F1-macro: 0.70
Prec-macro: 0.72  Rec-macro: 0.68
Recall(Long): 0.81  Precision(Short): 0.66


SHAP global importance:

first_response_minutes   >>> strongest driver
comment_count            >> medium influence
participants_count       >> medium influence
text_size                > minor influence


Causal Forest results:

Average Treatment Effect: -0.05%
Causal feature importances: comment_count â‰« participants_count > text_size

ğŸ“Œ Takeaways

Predictive ML models can classify ticket resolution times with good accuracy.

First response time is the strongest predictor.

But causal analysis shows its direct impact is smallâ€”most signal comes from related issue dynamics.

SHAP + causal inference ensure the model is both accurate and interpretable.

Deployment via FastAPI + Flask ensures flexibility:

FastAPI = scalable production API

Flask = simple dashboards / internal tooling

CI/CD with GitHub Actions and Azure Web App gives a full MLOps pipeline from training â†’ testing â†’ deployment.